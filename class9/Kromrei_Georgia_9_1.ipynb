{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris() # load iris data set\n",
    "\n",
    "x = iris.data[:,2:]\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 5)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linked_data = list(zip(x, y))\n",
    "shuffle(linked_data)\n",
    "x, y = zip(*linked_data)\n",
    "range(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.933333333333\n"
     ]
    }
   ],
   "source": [
    "fold = 5\n",
    "list_of_scores = []\n",
    "\n",
    "#This is to do all the things to the things\n",
    "for i in range(fold):\n",
    "    x_test = []\n",
    "    x_train = []\n",
    "    y_test = []\n",
    "    y_train = []\n",
    "    start = int(len(x)/fold*i)\n",
    "    stop = int((len(x)/fold*i)+(len(x)/fold))\n",
    "    x_test = x[start:stop]\n",
    "    x_train = x[stop:]+ x[:start]\n",
    "    y_test = y[start:stop]\n",
    "    y_train = y[stop:]+ y[:start]\n",
    "    \n",
    "    #Make a fit using the training data\n",
    "    dt = tree.DecisionTreeClassifier().fit(x_train,y_train)\n",
    "    \n",
    "    #Make a y prediction based on the test data\n",
    "    y_pred= dt.predict(x_test)\n",
    "    \n",
    "    #Compare the accuracy of prediction\n",
    "    score = metrics.accuracy_score(y_test, y_pred)\n",
    "    list_of_scores.append(score)\n",
    "    \n",
    "average_score = sum(list_of_scores)/len(list_of_scores)\n",
    "print(average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now see how it compares to sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94000000000000006"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(dt,x,y,cv=5)\n",
    "np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
